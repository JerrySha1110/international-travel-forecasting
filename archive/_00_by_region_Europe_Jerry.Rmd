---
title: "_00_by_region"
author: "Julian Kleindiek"
date: "7/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
library(tidyverse)
# library(kableExtra)
# library(janitor)
# library(maps)
# library(geosphere)
library(data.table)
library(forecast)
library(tseries)
library(fpp)
library(TSA)
library(jsonlite)
library(vars)

# set paths
path = "E:/UChicago/4 - Summer 2020/Times Series Analysis/Final Project/"
filename = "International_Report_Passengers.csv"
```


### Step 1. Read in data
```{r}
# set working directory
setwd(path)

# read in flight data 
df = fread(file.path(filename))

# save df as tibble
df = as_tibble(df)

# print available columns
names(df)

# head
head(df)
```

### Step 2. Data cleaning
```{r}
# to numeric
df$total = as.numeric(gsub(",", "", df$total))
df$scheduled = as.numeric(gsub(",", "", df$scheduled))
```


### Step 3. Data visualization
```{r}
# define regions
south_europe = c("PORTUGAL", "SPAIN", "MALTA", "CYPRUS", "FRANCE", "ITALY", "CROATIA", "GREECE")
southeast_asia = c("INDONESIA", "THAILAND", "MALAYSIA", "PHILIPPINES", "CAMBODIA") # "VIETNAM", "SINGAPUR", "LAOS"
south_america = c("BRAZIL", "ARGENTINA", "COLOMBIA", "PERU", "CHILE", "ECUADOR", "BOLIVIA")
oceania = c("AUSTRALIA", "NEW ZEALAND")
```

```{r}
# filter dat by given regions
df_south_europe = df %>% filter(fg_country %in% south_europe)
df_southeast_asia = df %>% filter(fg_country %in% southeast_asia)
df_south_america = df %>% filter(fg_country %in% south_america)
df_oceania = df %>% filter(fg_country %in% oceania)
```

```{r}
# aggregate by year
df_south_europe_y = aggregate(df_south_europe$total, by=list(cat1=df_south_europe$year), FUN=sum)
df_southeast_asia_y = aggregate(df_southeast_asia$total, by=list(cat1=df_southeast_asia$year), FUN=sum)
df_south_america_y= aggregate(df_south_america$total, by=list(cat1=df_south_america$year), FUN=sum)
df_oceania_y = aggregate(df_oceania$total, by=list(cat1=df_oceania$year), FUN=sum)

# create time series object
south_europe_ts_y = ts(df_south_europe_y$x, start=1990, end=2019, frequency = 1)
southeast_asia_ts_y = ts(df_southeast_asia_y$x, start=1990, end=2019, frequency = 1)
south_america_ts_y = ts(df_south_america_y$x, start=1990, end=2019, frequency = 1)
oceania_ts_y = ts(df_oceania_y$x, start=1990, end=2019, frequency = 1)

# plot
ts.plot(south_europe_ts_y, southeast_asia_ts_y, south_america_ts_y, oceania_ts_y,
        ylab="Number of passengers", xlab="Time", col=c("blue", "red", "green", "yellow"),
        main="International travel from the US by year")
legend("topleft", lty=c(1,1,1,1), col=c("blue", "red", "green", "yellow"),
       legend=c("South Europe", "Southeast Asia", "South America", "Oceania"))
```
\
- Note that 2019 data is not complete\


```{r}
# aggregate by month
df_south_europe_m = aggregate(df_south_europe$total, by=list(cat1=df_south_europe$year, 
                                                             cat2=df_south_europe$month), FUN=sum)
df_southeast_asia_m = aggregate(df_southeast_asia$total, by=list(cat1=df_southeast_asia$year, 
                                                             cat2=df_southeast_asia$month), FUN=sum)
df_south_america_m = aggregate(df_south_america$total, by=list(cat1=df_south_america$year, 
                                                             cat2=df_south_america$month), FUN=sum)
df_oceania_m = aggregate(df_oceania$total, by=list(cat1=df_oceania$year, 
                                                             cat2=df_oceania$month), FUN=sum)

# create time series object
south_europe_ts_m = ts(df_south_europe_m[order(df_south_europe_m$cat1),]$x, frequency = 12, start=c(1990,1))
southeast_asia_ts_m = ts(df_southeast_asia_m[order(df_southeast_asia_m$cat1),]$x, frequency = 12, start=c(1990,1))
south_america_ts_m = ts(df_south_america_m[order(df_south_america_m$cat1),]$x, frequency = 12, start=c(1990,1))
oceania_ts_m = ts(df_oceania_m[order(df_oceania_m$cat1),]$x, frequency = 12, start=c(1990,1))

# plot
ts.plot(south_europe_ts_m, southeast_asia_ts_m, south_america_ts_m, oceania_ts_m,
        ylab="Number of passengers", xlab="Time", col=c("blue", "red", "green", "yellow"),
        main="International travel from the US by month")
legend("topleft", lty=c(1,1,1,1), col=c("blue", "red", "green", "yellow"),
       legend=c("South Europe", "Southeast Asia", "South America", "Oceania"))
```


### Step 4. Data modeling
#### Pre-processing and insights
```{r}
# decompose data
fit_multi = decompose(south_europe_ts_m, type="multiplicative")

# plot decomposed data
plot(fit_multi)
```
\
- The appropriate Holt-Winters method for this data set is multiplicative, because the magnitude of the seasonal pattern increases over time\
- The additive Holt-Winters method would be used if the seasonal fluctuation does not change in magnitude over time\


```{r}
# ACF and PACF for data
tsdisplay(south_europe_ts_m, main="Monthly passenger travel from the US to South Europe")
```
\
- The data is non-stationary having a positive trend\
- Further, it follows a seasonal pattern with a non-constant variance over time\

```{r}
# plot with box-cox transformation
lambda = BoxCox.lambda(south_europe_ts_m)
south_europe_ts_m_trans = BoxCox(south_europe_ts_m, lambda=lambda)
tsdisplay(south_europe_ts_m_trans)
```
\
- The Box-Cox transformation leads to a more constant variance over time\

```{r}
# test for stationarity of training data with KPSS test
kpss.test(south_europe_ts_m, null="Level")
```
\
- The null hypothesis for the KPSS test is that the data is stationary\
- Large p-values are indicative of stationarity and small p-values suggest non-stationarity\
- For this data, the p-value is 0.01 and hence the data is non-stationary at a 5% significance level\

```{r}
# deseasonalize data
south_europe_ts_m_deseasonal = diff(diff(south_europe_ts_m, lag=12), lag=1)

# plot results
tsdisplay(south_europe_ts_m_deseasonal, main="ACF and PACF with seasonal and non-seasonal differencing")
```
\
- After deseasonalizing and detrending the data, we can observe no seasonality in the data anymore\
- In the PACF, there is a cutoff at lag 12 and 24 (P could be 1 or 2)\
- The ACF cutoff at lag 12 (Q could be 1)\

```{r}
# select test and training data
train = window(south_europe_ts_m, c(2008,1), c(2018,6))
test = window(south_europe_ts_m, c(2018,7))
```

#### Naive (Lola)

#### ETS (Lola)

#### Arima (Jerry) 

```{r}
# Build EACF table 
eacf(train,
     ar.max = 3,
     ma.max = 3)
```

```{r}
# Construct Arima models using the results fr
Arima(train,
      order = c(2,1,0))

Arima(train,
      order = c(2,1,1))

Arima(train,
      order = c(1,1,2))

Arima(train,
      order = c(1,1,3))

Arima(train,
      order = c(3,1,3))
```

Based on eacf(), ARIMA(1,1,3) has the best AICc

```{r}
arima_model <- Arima(train, 
                     order = c(1,1,3), 
                     seasonal = list(order = c(1,1,2), 
                                     period = 12),
                     include.drift = TRUE, 
                     lambda = lambda,
                     method = "ML")

checkresiduals(arima_model)
```


With trial and error, ARIMA(1,1,3)(1,1,2)[12] appears to be a good choice.

```{r}
plot(forecast(arima_model,
              h = 12),
              include = 50)
```

```{r}
arima_model_fc <- forecast(arima_model,
                           h = 12)
```

```{r}
accuracy(arima_model_fc, test)
```

```{r}
# save RMSE/MAE
arima_model_rmse_test = accuracy(arima_model_fc, test)["Test set", "RMSE"]
arima_model_mae_test = accuracy(arima_model_fc, test)["Test set", "MAE"]
```





#### auto.arima (Julian)
```{r}
# fit auto.arima() model
auto_arima_model = auto.arima(train, lambda=lambda, stationary=FALSE, seasonal=TRUE)

# summary
summary(auto_arima_model)
```

```{r}
# investigate residuals
checkresiduals(auto_arima_model)
```
\
- The residuals for the plots do not appear to be white noise\
- Based on the Ljung-Box test, we cannot accept the null hypothesis of the data to be independent since the p-value < 0.05\
- At lag 11 there is a spice in the ACF plot for the residuals\

```{r}
# forecast
h = 12
auto_arima_fc = forecast(auto_arima_model, h=h)
```

```{r}
# plot test data and forecasts of both models
autoplot(test, xlab = "Time", ylab = "Number of passengers",
         main="12-month forecast for South Europe Monthly with auto arima model", series="Test data") +
  autolayer(auto_arima_fc$mean, series="Forecast model_1")
```

```{r}
# plot the forecast
plot(auto_arima_fc, main="12-month forecast for South Europe Monthly with auto arima model")
```
\
- In this plot we can observe the multiple seasonality in the data for South Europe

```{r}
# accuracy
accuracy(auto_arima_fc, test)

# save RMSE/MAE
auto_arima_rmse_test = accuracy(auto_arima_fc, test)["Test set", "RMSE"]
auto_arima_rmse_test = accuracy(auto_arima_fc, test)["Test set", "MAE"]
```


#### VAR (Julian)
```{r}
# source https://datahub.io/core/cpi-us#r
json_file = 'https://datahub.io/core/cpi-us/datapackage.json'
json_data = fromJSON(paste(readLines(json_file), collapse=""))

# get list of all resources:
print(json_data$resources$name)

# print all tabular data(if exists any)
for(i in 1:length(json_data$resources$datahub$type)){
  if(json_data$resources$datahub$type[i]=='derived/csv'){
    path_to_file = json_data$resources$path[i]
    dat_cpi = read.csv(url(path_to_file))
    print(head(dat_cpi))
  }
}

# create year column
dat_cpi["year"] = substr(dat_cpi[,1], start = 1, stop = 4)

# create month column
dat_cpi["month"] = substr(dat_cpi[,1], start = 6, stop = 7)

# adjust type
dat_cpi["month"] = as.numeric(unlist(dat_cpi["month"]))
dat_cpi["year"] = as.numeric(unlist(dat_cpi["year"]))

# impute mean for nas in cpi and inflation
dat_cpi[is.na(dat_cpi[,2]), 2] <- mean(dat_cpi[,2], na.rm = TRUE)
dat_cpi[is.na(dat_cpi[,3]), 3] <- mean(dat_cpi[,3], na.rm = TRUE)

# create ts object for cpi
cpi_ts = ts(dat_cpi["Index"], frequency = 12, start=c(1990,1), end=c(2019,6))

# create ts object for inflation
infl_ts = ts(dat_cpi["Inflation"], frequency = 12, start=c(1990,1), end=c(2019,6))
```

```{r}
# plot cpi and inflation
plot(cpi_ts, main="CPI in the US from 1900 to 2020")
plot(infl_ts, main="Inflation in the US from 1900 to 2020")
```

```{r}
# ensure stationarity of variables
cpi_ts_stat = diff(cpi_ts)
infl_ts_stat = diff(infl_ts)
south_europe_ts_m_stat = diff(south_europe_ts_m)

# plot new variables
plot(cpi_ts_stat)
plot(infl_ts_stat)
plot(south_europe_ts_m_stat)
```


```{r}
# select test and training data
train_travel = window(south_europe_ts_m_stat, c(2008,1), c(2018,6))
test_travel = window(south_europe_ts_m_stat, c(2018,7))
train_cpi = window(cpi_ts_stat, c(2008,1), c(2018,6))
test_cpi = window(cpi_ts_stat, c(2018,7))
train_infl = window(infl_ts_stat, c(2008,1), c(2018,6))
test_infl = window(infl_ts_stat, c(2018,7))

# var order selection for number of passengers
VARselect(cbind(south_europe_ts_m_stat, cpi_ts_stat, infl_ts_stat), lag.max = 15, type = "const")$selection

# build model
var_model_1 = VAR(cbind(train_travel, train_cpi, train_infl), p=4, type="both", season=12)
summary(var_model_1)
var_model_2 = VAR(cbind(train_travel, train_cpi, train_infl), p=12, type="both", season=12)
summary(var_model_2)
```
\
- Based on the AIC, the VAR(12) should be selected\

```{r}
# test serial correlation in the residuals
serial.test(var_model_1, lags.pt = 10, type = "PT.asymptotic")

# plot acf of residuals
varresids_1 = residuals(var_model_1)
acf(varresids_1[,1], main="Residuals Number Passengers")
acf(varresids_1[,2], main="Residuals CPI")
acf(varresids_1[,3], main="Residuals Inflation")
acf(varresids_1)

# test serial correlation in the residuals
serial.test(var_model_2, lags.pt = 10, type = "PT.asymptotic")

# plot acf of residuals
varresids_2 = residuals(var_model_2)
acf(varresids_2[,1], main="Residuals Number Passengers")
acf(varresids_2[,2], main="Residuals CPI")
acf(varresids_2[,3], main="Residuals Inflation")
acf(varresids_2)
```
\
- The null hypothesis of the serial.test is that there is no serial correlation in the residuals\
- For a VAR(12) model, the null hypothesis of the serial.test is rejected\
- The ACF for the residuals of the number of passengers also shows that there is some pattern in the residuals\
- A VAR(4) model results in the ability to not reject the null hypothesis for residuals with serial correlation with a p-value > 0.05, but the ACF plot still suggest some pattern in the residuals\

```{r}
# forecast
h = 12
var_fc_1 = forecast(var_model_1, h=h)
var_fc_2 = forecast(var_model_2, h=h)
```

```{r}
# plot test data and forecasts 
autoplot(test_travel, xlab = "Time", ylab = "Number of passengers",
         main="12-month forecast for South Europe Monthly with VAR(4) model", series="Test data") +
  autolayer(var_fc_1$forecast$train_travel$mean, series="Forecast VAR(4) model")
```

```{r}
# plot the forecast
plot(var_fc_1)
```


```{r}
# accuracy
accuracy(var_fc_1$forecast$train_travel, test_travel)

# save RMSE/MAE
var_rmse_test = accuracy(var_fc_1$forecast$train_travel, test_travel)["Test set", "RMSE"]
var_rmse_test = accuracy(var_fc_1$forecast$train_travel, test_travel)["Test set", "MAE"]
```


#### TBATS (Lola)
```{r}
# fit tbats model
tbats_model = tbats(train)

# summary
summary(tbats_model)

# investigate residuals
checkresiduals(tbats_model)
```

#### Multiple seasonality (Kelley)
```{r}
# fit auto.arima model with fourier
multiple_seasonality_model = auto.arima(train, xreg=fourier(train, K=5), seasonal=FALSE)

# summary
summary(multiple_seasonality_model)

# investigate residuals
checkresiduals(multiple_seasonality_model)
```


```{r}
# forecast into the future
multiple_seasonality_fc = forecast(multiple_seasonality_model, xreg = fourier(train, K=5, h=24), level=c(80, 95))

# plot the forecast
plot(multiple_seasonality_fc, 
     main="24-month forecast for South Europe Monthly with auto arima model with fourier term")
```










