---
title: "_00_by_region"
author: "Julian Kleindiek"
date: "7/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
library(tidyverse)
library(kableExtra)
library(janitor)
library(maps)
library(geosphere)
library(data.table)
library(forecast)
library(tseries)
library(fpp)
library(TSA)
library(jsonlite)
library(vars)

# set paths
path = "/Users/juliankleindiek/Desktop/UChicago/08_Quarter4/02_TimeSeries/Project"
filename = "_01_International_Report_Passengers.csv"
```


### Step 1. Read in data
```{r}
# set working directory
setwd(path)

# read in flight data 
df = fread(file.path(filename))

# save df as tibble
df = as_tibble(df)

# print available columns
names(df)

# head
head(df)
```

### Step 2. Data cleaning
```{r}
# to numeric
df$total = as.numeric(gsub(",", "", df$total))
df$scheduled = as.numeric(gsub(",", "", df$scheduled))
```


### Step 3. Data visualization
```{r}
# define regions
south_europe = c("PORTUGAL", "SPAIN", "MALTA", "CYPRUS", "FRANCE", "ITALY", "CROATIA", "GREECE")
southeast_asia = c("INDONESIA", "THAILAND", "MALAYSIA", "PHILIPPINES", "CAMBODIA") # "VIETNAM", "SINGAPUR", "LAOS"
south_america = c("BRAZIL", "ARGENTINA", "COLOMBIA", "PERU", "CHILE", "ECUADOR", "BOLIVIA")
oceania = c("AUSTRALIA", "NEW ZEALAND")
```

```{r}
# filter dat by given regions
df_south_europe = df %>% filter(fg_country %in% south_europe)
df_southeast_asia = df %>% filter(fg_country %in% southeast_asia)
df_south_america = df %>% filter(fg_country %in% south_america)
df_oceania = df %>% filter(fg_country %in% oceania)
```

```{r}
# aggregate by year
df_south_europe_y = aggregate(df_south_europe$total, by=list(cat1=df_south_europe$year), FUN=sum)
df_southeast_asia_y = aggregate(df_southeast_asia$total, by=list(cat1=df_southeast_asia$year), FUN=sum)
df_south_america_y= aggregate(df_south_america$total, by=list(cat1=df_south_america$year), FUN=sum)
df_oceania_y = aggregate(df_oceania$total, by=list(cat1=df_oceania$year), FUN=sum)

# create time series object
south_europe_ts_y = ts(df_south_europe_y$x, start=1990, end=2019, frequency = 1)
southeast_asia_ts_y = ts(df_southeast_asia_y$x, start=1990, end=2019, frequency = 1)
south_america_ts_y = ts(df_south_america_y$x, start=1990, end=2019, frequency = 1)
oceania_ts_y = ts(df_oceania_y$x, start=1990, end=2019, frequency = 1)

# plot
ts.plot(south_europe_ts_y, southeast_asia_ts_y, south_america_ts_y, oceania_ts_y,
        ylab="Number of passengers", xlab="Time", col=c("blue", "red", "green", "yellow"),
        main="International travel from the US by year")
legend("topleft", lty=c(1,1,1,1), col=c("blue", "red", "green", "yellow"),
       legend=c("South Europe", "Southeast Asia", "South America", "Oceania"))
```
\
- Note that 2019 data is not complete\


```{r}
# aggregate by month
df_south_europe_m = aggregate(df_south_europe$total, by=list(cat1=df_south_europe$year, 
                                                             cat2=df_south_europe$month), FUN=sum)
df_southeast_asia_m = aggregate(df_southeast_asia$total, by=list(cat1=df_southeast_asia$year, 
                                                             cat2=df_southeast_asia$month), FUN=sum)
df_south_america_m = aggregate(df_south_america$total, by=list(cat1=df_south_america$year, 
                                                             cat2=df_south_america$month), FUN=sum)
df_oceania_m = aggregate(df_oceania$total, by=list(cat1=df_oceania$year, 
                                                             cat2=df_oceania$month), FUN=sum)

# create time series object
south_europe_ts_m = ts(df_south_europe_m[order(df_south_europe_m$cat1),]$x, frequency = 12, start=c(1990,1))
southeast_asia_ts_m = ts(df_southeast_asia_m[order(df_southeast_asia_m$cat1),]$x, frequency = 12, start=c(1990,1))
south_america_ts_m = ts(df_south_america_m[order(df_south_america_m$cat1),]$x, frequency = 12, start=c(1990,1))
oceania_ts_m = ts(df_oceania_m[order(df_oceania_m$cat1),]$x, frequency = 12, start=c(1990,1))

# plot
ts.plot(south_europe_ts_m, southeast_asia_ts_m, south_america_ts_m, oceania_ts_m,
        ylab="Number of passengers", xlab="Time", col=c("blue", "red", "green", "yellow"),
        main="International travel from the US by month")
legend("topleft", lty=c(1,1,1,1), col=c("blue", "red", "green", "yellow"),
       legend=c("South Europe", "Southeast Asia", "South America", "Oceania"))
```


### Step 4. Data modeling
#### Pre-processing and insights
```{r}
# decompose data
fit_multi = decompose(south_europe_ts_m, type="multiplicative")

# plot decomposed data
plot(fit_multi)
```
\
- The appropriate Holt-Winters method for this data set is multiplicative, because the magnitude of the seasonal pattern increases over time\
- The additive Holt-Winters method would be used if the seasonal fluctuation does not change in magnitude over time\


```{r}
# ACF and PACF for data
tsdisplay(south_europe_ts_m, main="Monthly passenger travel from the US to South Europe")
```
\
- The data is non-stationary having a positive trend\
- Further, it follows a seasonal pattern with a non-constant variance over time\

```{r}
# plot with box-cox transformation
lambda = BoxCox.lambda(south_europe_ts_m)
south_europe_ts_m_trans = BoxCox(south_europe_ts_m, lambda=lambda)
tsdisplay(south_europe_ts_m_trans)
```
\
- The Box-Cox transformation leads to a more constant variance over time\

```{r}
# test for stationarity of training data with KPSS test
kpss.test(south_europe_ts_m, null="Level")
```
\
- The null hypothesis for the KPSS test is that the data is stationary\
- Large p-values are indicative of stationarity and small p-values suggest non-stationarity\
- For this data, the p-value is 0.01 and hence the data is non-stationary at a 5% significance level\

```{r}
# deseasonalize data
south_europe_ts_m_deseasonal = diff(south_europe_ts_m, lag=12)

# plot results
tsdisplay(south_europe_ts_m_deseasonal)
```
\
- After deseasonalizing the data, we can observe no seasonality in the data anymore\
- In the PACF, there is a cutoff at lag 1 and at lag 12 allowing to draw the conclusion that p and P could both be 1\
- The ACF is now decaying, which it wasnâ€™t prior to the seasonal differencing and which is a good sign\

```{r}
# select test and training data
train = window(south_europe_ts_m, c(2008,1), c(2018,6))
test = window(south_europe_ts_m, c(2018,7))
```

#### Naive (Lola)

#### ETS (Lola)

#### Arima (Jerry) 

#### auto.arima (Julian)
```{r}
# fit auto.arima() model
auto_arima_model = auto.arima(train, stationary=FALSE, seasonal=TRUE)

# summary
summary(auto_arima_model)
```

```{r}
# investigate residuals
checkresiduals(auto_arima_model)
```
\
- The residuals for the plots do not appear to be white noise\
- However, based on the Ljung-Box test, we cannot accept the null hypothesis of the data to be independent  since the p-value < 0.05\
- At lag 11 there is a spice in the ACF plot for the residuals\

```{r}
# forecast
h = 12
auto_arima_fc = forecast(auto_arima_model, h=h)
```

```{r}
# plot test data and forecasts of both models
autoplot(test, xlab = "Time", ylab = "Number of passengers",
         main="12-month forecast for South Europe Monthly with auto arima model", series="Test data") +
  autolayer(auto_arima_fc$mean, series="Forecast model_1")
```

```{r}
# plot the forecast
plot(auto_arima_fc, main="12-month forecast for South Europe Monthly with auto arima model")
```
\
- In this plot we can observe the multiple seasonality in the data for South Europe

```{r}
# accuracy
accuracy(auto_arima_fc, test)

# save RMSE/MAE
auto_arima_rmse_test = accuracy(auto_arima_fc, test)["Test set", "RMSE"]
auto_arima_rmse_test = accuracy(auto_arima_fc, test)["Test set", "MAE"]
```


#### VAR (Julian)
```{r}
# source https://datahub.io/core/cpi-us#r
json_file = 'https://datahub.io/core/cpi-us/datapackage.json'
json_data = fromJSON(paste(readLines(json_file), collapse=""))

# get list of all resources:
print(json_data$resources$name)

# print all tabular data(if exists any)
for(i in 1:length(json_data$resources$datahub$type)){
  if(json_data$resources$datahub$type[i]=='derived/csv'){
    path_to_file = json_data$resources$path[i]
    dat_cpi = read.csv(url(path_to_file))
    print(head(dat_cpi))
  }
}

# create year column
dat_cpi["year"] = substr(dat_cpi[,1], start = 1, stop = 4)

# create month column
dat_cpi["month"] = substr(dat_cpi[,1], start = 6, stop = 7)

# adjust type
dat_cpi["month"] = as.numeric(unlist(dat_cpi["month"]))
dat_cpi["year"] = as.numeric(unlist(dat_cpi["year"]))

# impute mean for nas in cpi and inflation
dat_cpi[is.na(dat_cpi[,2]), 2] <- mean(dat_cpi[,2], na.rm = TRUE)
dat_cpi[is.na(dat_cpi[,3]), 3] <- mean(dat_cpi[,3], na.rm = TRUE)

# create ts object for cpi
cpi_ts = ts(dat_cpi["Index"], frequency = 12, start=c(1990,1), end=c(2019,6))

# create ts object for inflation
infl_ts = ts(dat_cpi["Inflation"], frequency = 12, start=c(1990,1), end=c(2019,6))
```

```{r}
# plot cpi and inflation
plot(cpi_ts, main="CPI in the US from 1900 to 2020")
plot(infl_ts, main="Inflation in the US from 1900 to 2020")
```

```{r}
# select test and training data
train_cpi = window(cpi_ts, c(2008,1), c(2018,6))
test_cpi = window(cpi_ts, c(2018,7))

# var order selection for number of passengers
VARselect(cbind(south_europe_ts_m, cpi_ts), lag.max = 15, type = "const")$selection

# build model
var_model_1 = VAR(cbind(train, train_cpi), p=4, type="both", season=12)
var_model_2 = VAR(cbind(train, train_cpi), p=13, type="both", season=12)

# forecast
h = 12
var_fc_1 = forecast(var_model_1, h=h)
var_fc_2 = forecast(var_model_2, h=h)
```
\
- Based on the AIC, the VAR(13) should be selected\

```{r}
# test serial correlation in the residuals
serial.test(var_model_1, lags.pt = 10, type = "PT.asymptotic")

# plot acf of residuals
varresids_1 = residuals(var_model_1)
acf(varresids_1[,1], main="Residuals Number Passengers")
acf(varresids_1[,2], main="Residuals CPI")

# test serial correlation in the residuals
serial.test(var_model_2, lags.pt = 10, type = "PT.asymptotic")

# plot acf of residuals
varresids_2 = residuals(var_model_2)
acf(varresids_2[,1], main="Residuals Number Passengers")
acf(varresids_2[,2], main="Residuals CPI")
```
\
- The null hypothesis of the serial.test is that there is no serial correlation in the residuals\
- For a VAR(13) model, the null hypothesis of the serial.test is rejected\
- The ACF for the residuals of the number of passengers also shows that there is some pattern in the residuals\
- A VAR(4) model results in the ability to not reject the null hypothesis for residuals with serial correlation with a p-value > 0.05, but the ACF plot still suggest some pattern in the residuals\

```{r}
# plot test data and forecasts 
autoplot(test, xlab = "Time", ylab = "Number of passengers",
         main="12-month forecast for South Europe Monthly with VAR(4) model", series="Test data") +
  autolayer(var_fc_1$forecast$train$mean, series="Forecast VAR(4) model")
```


```{r}
# accuracy
accuracy(var_fc_1$forecast$train, test)

# save RMSE/MAE
var_rmse_test = accuracy(var_fc_1$forecast$train, test)["Test set", "RMSE"]
var_rmse_test = accuracy(var_fc_1$forecast$train, test)["Test set", "MAE"]
```


#### TBATS (Lola)
```{r}
# fit tbats model
tbats_model = tbats(train)

# summary
summary(tbats_model)

# investigate residuals
checkresiduals(tbats_model)
```

#### Multiple seasonality (Kelley)
```{r}
# fit auto.arima model with fourier
multiple_seasonality_model = auto.arima(train, xreg=fourier(train, K=5), seasonal=FALSE)

# summary
summary(multiple_seasonality_model)

# investigate residuals
checkresiduals(multiple_seasonality_model)
```
\
- Compared to model_1, the p-value for the Ljung-Box test is higher, meaning that the model is better in capturing the patterns in the residuals, but it is not capturing it sufficiently well since we can still not accept the null hypothesis of uncorrelated residuals\

```{r}
# forecast into the future
multiple_seasonality_fc = forecast(multiple_seasonality_model, xreg = fourier(train, K=5, h=24), level=c(80, 95))

# plot the forecast
plot(multiple_seasonality_fc, 
     main="24-month forecast for South Europe Monthly with auto arima model with fourier term")
```










