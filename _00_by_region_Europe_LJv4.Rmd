---
title: "_00_by_region"
author: "Julian Kleindiek"
date: "7/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
library(tidyverse)
library(kableExtra)
library(janitor)
library(maps)
library(geosphere)
library(data.table)
library(forecast)
library(tseries)
library(fpp)
#library(TSA)
library(jsonlite)
library(vars)

# set paths
path = "/Users/lolajohnston/Documents/001_school/004_summer_2020/004_time_series/005_final_project"
filename = "_01_International_Report_Passengers.csv"
```


### Step 1. Read in data
```{r}
# set working directory
setwd(path)

# read in flight data 
df = fread(file.path(filename))

# save df as tibble
df = as_tibble(df)

# print available columns
names(df)

# head
head(df)
```

### Step 2. Data cleaning
```{r}
# to numeric
df$total = as.numeric(gsub(",", "", df$total))
df$scheduled = as.numeric(gsub(",", "", df$scheduled))
```


### Step 3. Data visualization
```{r}
# define regions
south_europe = c("PORTUGAL", "SPAIN", "MALTA", "CYPRUS", "FRANCE", "ITALY", "CROATIA", "GREECE")
southeast_asia = c("INDONESIA", "THAILAND", "MALAYSIA", "PHILIPPINES", "CAMBODIA") # "VIETNAM", "SINGAPUR", "LAOS"
south_america = c("BRAZIL", "ARGENTINA", "COLOMBIA", "PERU", "CHILE", "ECUADOR", "BOLIVIA")
oceania = c("AUSTRALIA", "NEW ZEALAND")
```

```{r}
# filter dat by given regions
df_south_europe = df %>% filter(fg_country %in% south_europe)
df_southeast_asia = df %>% filter(fg_country %in% southeast_asia)
df_south_america = df %>% filter(fg_country %in% south_america)
df_oceania = df %>% filter(fg_country %in% oceania)
```

```{r}
# aggregate by year
df_south_europe_y = aggregate(df_south_europe$total, by=list(cat1=df_south_europe$year), FUN=sum)
df_southeast_asia_y = aggregate(df_southeast_asia$total, by=list(cat1=df_southeast_asia$year), FUN=sum)
df_south_america_y= aggregate(df_south_america$total, by=list(cat1=df_south_america$year), FUN=sum)
df_oceania_y = aggregate(df_oceania$total, by=list(cat1=df_oceania$year), FUN=sum)

# create time series object
south_europe_ts_y = ts(df_south_europe_y$x, start=1990, end=2019, frequency = 1)
southeast_asia_ts_y = ts(df_southeast_asia_y$x, start=1990, end=2019, frequency = 1)
south_america_ts_y = ts(df_south_america_y$x, start=1990, end=2019, frequency = 1)
oceania_ts_y = ts(df_oceania_y$x, start=1990, end=2019, frequency = 1)

# plot
ts.plot(south_europe_ts_y, southeast_asia_ts_y, south_america_ts_y, oceania_ts_y,
        ylab="Number of passengers", xlab="Time", col=c("blue", "red", "green", "yellow"),
        main="International travel from the US by year")
legend("topleft", lty=c(1,1,1,1), col=c("blue", "red", "green", "yellow"),
       legend=c("South Europe", "Southeast Asia", "South America", "Oceania"))
```
\
- Note that 2019 data is not complete\


```{r}
# aggregate by month
df_south_europe_m = aggregate(df_south_europe$total, by=list(cat1=df_south_europe$year, 
                                                             cat2=df_south_europe$month), FUN=sum)
df_southeast_asia_m = aggregate(df_southeast_asia$total, by=list(cat1=df_southeast_asia$year, 
                                                             cat2=df_southeast_asia$month), FUN=sum)
df_south_america_m = aggregate(df_south_america$total, by=list(cat1=df_south_america$year, 
                                                             cat2=df_south_america$month), FUN=sum)
df_oceania_m = aggregate(df_oceania$total, by=list(cat1=df_oceania$year, 
                                                             cat2=df_oceania$month), FUN=sum)

# create time series object
south_europe_ts_m = ts(df_south_europe_m[order(df_south_europe_m$cat1),]$x, frequency = 12, start=c(1990,1))
southeast_asia_ts_m = ts(df_southeast_asia_m[order(df_southeast_asia_m$cat1),]$x, frequency = 12, start=c(1990,1))
south_america_ts_m = ts(df_south_america_m[order(df_south_america_m$cat1),]$x, frequency = 12, start=c(1990,1))
oceania_ts_m = ts(df_oceania_m[order(df_oceania_m$cat1),]$x, frequency = 12, start=c(1990,1))

# plot
ts.plot(south_europe_ts_m, southeast_asia_ts_m, south_america_ts_m, oceania_ts_m,
        ylab="Number of passengers", xlab="Time", col=c("blue", "red", "green", "yellow"),
        main="International travel from the US by month")
legend("topleft", lty=c(1,1,1,1), col=c("blue", "red", "green", "yellow"),
       legend=c("South Europe", "Southeast Asia", "South America", "Oceania"))
```
```{r}


```



```{r}

```


### Step 4. Data modeling
#### Pre-processing and insights
```{r}
# decompose data
fit_multi = decompose(south_europe_ts_m, type="multiplicative")

# plot decomposed data
plot(fit_multi)
```
\
- The appropriate Holt-Winters method for this data set is multiplicative, because the magnitude of the seasonal pattern increases over time\
- The additive Holt-Winters method would be used if the seasonal fluctuation does not change in magnitude over time\


```{r}
# ACF and PACF for data
tsdisplay(south_europe_ts_m, main="Monthly passenger travel from the US to South Europe")
```
\
- The data is non-stationary having a positive trend\
- Further, it follows a seasonal pattern with a non-constant variance over time\

```{r}
# plot with box-cox transformation
lambda = BoxCox.lambda(south_europe_ts_m)
south_europe_ts_m_trans = BoxCox(south_europe_ts_m, lambda=lambda)
tsdisplay(south_europe_ts_m_trans)
```
\
- The Box-Cox transformation leads to a more constant variance over time\

```{r}
# test for stationarity of training data with KPSS test
kpss.test(south_europe_ts_m, null="Level")
```
\
- The null hypothesis for the KPSS test is that the data is stationary\
- Large p-values are indicative of stationarity and small p-values suggest non-stationarity\
- For this data, the p-value is 0.01 and hence the data is non-stationary at a 5% significance level\

```{r}
# deseasonalize data
south_europe_ts_m_deseasonal = diff(south_europe_ts_m, lag=12)

# plot results
tsdisplay(south_europe_ts_m_deseasonal)
```
\
- After deseasonalizing the data, we can observe no seasonality in the data anymore\
- In the PACF, there is a cutoff at lag 1 and at lag 12 allowing to draw the conclusion that p and P could both be 1\
- The ACF is now decaying, which it wasn’t prior to the seasonal differencing and which is a good sign\

```{r}
# select test and training data
train = window(south_europe_ts_m, c(2008,1), c(2018,6))
test = window(south_europe_ts_m, c(2018,7))
```

###  Start Lola's Section 

Forecasts
- Average 
- Naive 
- Seasonal Naive
- Random Walk w/ Drift
- Holt Winters
- Simple Exponential Smoothing
- ETS()
- TBATS()

```{r}

#######################
## make data stationary
#######################

# plot the data
autoplot(south_europe_ts_m)

# take difference of 1 to remove the trend
diff_data <- diff(south_europe_ts_m, difference = 1)
# plot results
tsdisplay(diff_data)

# take seasonal difference to remove seasonality
diff_data_lag12 <- diff(south_europe_ts_m, lag = 12)
tsdisplay(diff_data_lag12)

# still not stationary so take another difference
diff_data_2 <- diff(diff_data_lag12, lag = 1)
tsdisplay(diff_data_2)

# check for stationarity
adf.test(diff_data_2) 

# select test and training data
train_diff = window(diff_data_2, c(2008,1), c(2018,6))
test_diff = window(diff_data_2, c(2018,7))
```

the p-value is less than .05, we accept the null hypothesis that the data is stationary.


```{r}

###########################################################
## make data stationary w box cox.. not as good.
###########################################################

# normalizing data through box cox transformation

autoplot(south_europe_ts_m)
lambda <- BoxCox.lambda(south_europe_ts_m) # lambda = -0.002124992 barely needs transformation 

boxcox_data <- train %>% BoxCox(lambda = lambda) 
autoplot(boxcox_data) + ggtitle("Box Cox Transformation")

# check for stationarity
kpss.test(boxcox_data, null = "Trend") #  p-value is 0.1 which would suggest stationary bc it is greater than > 0.05
kpss.test(boxcox_data, null = "Level") # p-value is > 0.058 which is just barely greater than 0.05 so we don't want to accept this.
tsdisplay(boxcox_data)

# box cox isn't a good option to use in this case. 
```


####  Average Forecast (Lola)
```{r}
# Average forecast 
avg_fc <- meanf(train, h=length(test))

# plot and summarize the forecast
autoplot(avg_fc) + ggtitle("Average Forecast")
summary(avg_fc)

# check residuals
checkresiduals(avg_fc)

# check accuracy
accuracy(avg_fc, test)

#                       ME     RMSE      MAE       MPE     MAPE     MASE    
# Training set -8.123646e-11 332825.3 279369.3 -10.77332 29.71691 4.497258 
# Test set      4.678687e+05 637974.2 516031.6  23.90382 29.43255 8.307026 
```

Average Forecast = Poor performance

####  Naive Forecast (Lola)

```{r}
# naive() forecast 
naive_fc <- naive(train, h=length(test))

# plot and summarize the forecast
autoplot(naive_fc) + ggtitle("Naive Forecast")
summary(naive_fc)

# check residuals
checkresiduals(naive_fc)

# check accuracy
accuracy(naive_fc, test)

```

Naive Forecast = Poor performance

##### Cross Validation

```{r}

# Compute cross-validated errors
e <- tsCV(train, forecastfunction = naive, h = length(test))

# Compute the MAE values and remove missing values

### DOUBLE CHECK THIS IS THE CORRECT WAY TO CALCULATE MAE
mae <- colMeans(abs(e), na.rm = TRUE)
data.frame(h = 1:length(test), MAE = mae) %>%
  ggplot(aes(x = h, y = MAE)) + geom_point() + ggtitle("Naive MAE")

# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = TRUE)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:length(test), MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point() + ggtitle("Naive MSE")

```

Naive forecast uses the last point recorded.
ACF shows that there is seasonality and the residuals are far from white noise.
Terrible performance.


####  Seasonal Naive Forecast (Lola)
```{r}
# seasaonal naive() forecast 
snaive_fc <- snaive(train, h=length(test))

# plot and summarize the forecast
autoplot(snaive_fc) + ggtitle("Seasonal Naive Forecast")
summary(snaive_fc)

# check residuals
checkresiduals(snaive_fc)

# check accuracy
accuracy(snaive_fc, south_europe_ts_m)

#                    ME      RMSE      MAE       MPE      MAPE     MASE      ACF1 Theil's U
# Training set  41326.71  78756.18  62119.9  3.143089  5.616763 1.000000 0.6741741        NA
# Test set     167200.42 172991.76 167200.4 11.130821 11.130821 2.691576 0.3038452 0.5281209


```

Seasonal Naive forecast uses previous cycle (last year by month) for predicted values. 
ACF shows exponential decay, slightly better than plain naive forecast however the residuals are still far from white noise. 

Seasonal Naive Forecast = Poor performance.

#### Random Walk with Drift Forecast

```{r}
# random walk with drift forecast
rwf_fc <- rwf(train, h = length(test), drift = T)

# plot and summarize the forecast
autoplot(rwf_fc) + ggtitle("Random Walk with Drift Forecast")
summary(rwf_fc)

# check residuals
checkresiduals(rwf_fc)

# check accuracy
accuracy(rwf_fc, south_europe_ts_m)
#                        ME     RMSE      MAE        MPE     MAPE     MASE      ACF1 Theil's U
# Training set -1.210719e-11 195455.3 167016.8  -2.445516 17.63444 2.688620 0.3558061        NA
# Test set     -4.656280e+05 640270.1 514097.9 -42.756444 45.07686 8.275897 0.6772744  2.824434

```

This prediction understands the trend but lacks the seasonality pattern.
Random Walk w/ Drift = Poor Performance

```{r}
# holt winters forecast
hw_fc <- forecast(HoltWinters(train), h = length(test))

# plot and summarize the forecast
autoplot(hw_fc) + ggtitle("Holt Winters Forecast")
summary(hw_fc)

# check residuals
checkresiduals(hw_fc)

# check accuracy
accuracy(hw_fc, south_europe_ts_m)

#                  ME     RMSE      MAE         MPE     MAPE      MASE      ACF1 Theil's U
# Training set  7103.252 44813.54 34964.08 -0.05465902 3.354402 0.5628483 0.3044779        NA
# Test set     30355.328 60177.20 54110.42  1.19816985 3.499643 0.8710641 0.4702240 0.1893337

```

Holt Winters - Performance = Not bad


##### Simple Exponential Smoothing

```{r}
# simple exponential smoothing
ses_fc <- ses(train, alpha = .2, h = length(test))

# plot and summarize the forecast
autoplot(ses_fc) + ggtitle("Simple Exponential Smooothing Forecast")
summary(ses_fc)

# check residuals
checkresiduals(ses_fc)

# check accuracy
accuracy(ses_fc, south_europe_ts_m)
#                    ME     RMSE      MAE       MPE     MAPE     MASE      ACF1 Theil's U
#Training set  20564.17 319303.5 276101.2 -7.106620 28.65632 4.444650 0.7767461        NA
#Test set     128962.35 452483.4 410944.1 -0.257449 28.05403 6.615338 0.6669486  1.381126
```

SES is suitable for data with no trend or seasonal pattern.
Poor Performance.


##### Simple Exponential Smoothing with stationary data
Let's see what happens when we remove the trend and seasonality in our data and try SES

```{r}

# try SES on stationary data

# simple exponential smoothing, standard starting alpha = 0.2
ses_fc_diff <- ses(train_diff, alpha = .2, h = length(test_diff))

# plot and summarize the forecast
autoplot(ses_fc_diff) + ggtitle("Simple Exponential Smooothing Forecast on Stationary data")
summary(ses_fc_diff)

# check residuals
checkresiduals(ses_fc_diff)

# check accuracy
accuracy(ses_fc_diff, diff_data_2)

#                     ME     RMSE      MAE      MPE     MAPE      MASE       ACF1 Theil's U
# Training set   777.2056 56537.35 40856.09 85.92718 154.3045 0.6624570 -0.5087667        NA
# Test set     -7111.4212 48048.62 40609.14 97.16289 134.2245 0.6584528 -0.3618665  1.075371

```


Used the standard 0.2 = alpha, lets try to find the optimal alpha.

```{r}

# identify optimal alpha parameter
alpha <- seq(.01, .99, by = .01)
RMSE <- NA
for(i in seq_along(alpha)) {
  fit <- ses(train_diff, alpha = alpha[i], h = length(test_diff))
  RMSE[i] <- accuracy(fit, test_diff)[2,2]
}

# convert to a data frame and idenitify min alpha value
alpha.fit <- data_frame(alpha, RMSE)
alpha.min <- filter(alpha.fit, RMSE == min(RMSE))

# plot RMSE vs. alpha
ggplot(alpha.fit, aes(alpha, RMSE)) +
  geom_line() +
  geom_point(data = alpha.min, aes(alpha, RMSE), size = 2, color = "blue")  

alpha.min # 0.78	

```

```{r}
# refit model with alpha = .0.78
ses_2 <- ses(train_diff, alpha = 0.78, h = length(test_diff))

# check residuals and one-step forecast errors from the ETS(M,A,M) damped model.
cbind('Residuals' = residuals(ses_2),
      'Forecast errors' = residuals(fit,type='response')) %>%
  autoplot(facet=TRUE) + xlab("Year") + ylab("")

# check residuals
checkresiduals(ses_2)

ses_2 %>%autoplot(facets = TRUE) + ggtitle("South EU Passengers | 1 YR Forecast | SES alpha 0.78") + ylab("Passengers") + xlab("Time")

# check accuracy
accuracy(ses_2, diff_data_2)

#                    ME     RMSE      MAE      MPE      MAPE      MASE       ACF1 Theil's U
# Training set 302.86666 76738.01 56070.32 90.91896 353.22107 0.9091466 -0.6415545        NA
# Test set      52.94388 47519.47 38226.88 99.12958  99.12958 0.6198259 -0.3618665 0.8958322

```

Looks like this model is overfit.


#### ETS() 

```{r}
autoplot(decompose(south_europe_ts_m))
```

Decomposition shows us the patterns of our ETS we can then choose 

- error: additive (“A”), multiplicative (“M”), unknown (“Z”)
- trend: none (“N”), additive (“A”), multiplicative (“M”), unknown (“Z”)
- seasonality: none (“N”), additive (“A”), multiplicative (“M”), unknown (“Z”)

If you want the model to select the best option, use = "ZZZ" and the “optimal” model will be selected.

# ETS Holt Winters Method (aka Error, Trend, Seasonality
)
```{r}

### Additive, Additive, Additive
ets_aaa <- ets(train, model = "AAA")
autoplot(forecast(ets_aaa))+ ggtitle("ETS - AAA")
summary(ets_aaa)

### Multiplicative Multiplicative Multiplicative
ets_mmm <- ets(train, model = "MMM")
autoplot(forecast(ets_mmm))+ ggtitle("ETS - MMM")
summary(ets_mmm)

### Multiplicative, Additive, Additive
ets_maa <- ets(train, model = "MAA")
autoplot(forecast(ets_maa))+ ggtitle("ETS - MAA")
summary(ets_maa)

### Additive, Additive, Multiplicative
ets_aam <- ets(train, model = "AAM",  restrict = FALSE)
autoplot(forecast(ets_aam))+ ggtitle("ETS - AAM")
summary(ets_aam)

###  Multiplicative, Additive, Multiplicative
ets_mam <- ets(train, model = "MAM")
autoplot(forecast(ets_mam))+ ggtitle("ETS - MAM")
summary(ets_mam)

### Optimal Model
ets_fc_opt <- ets(train, model = "ZZZ") # setting model to "ZZZ" will give you the optimal parameters

autoplot(forecast(ets_fc_opt))+ ggtitle("ETS(M,Ad,M)")
summary(ets_fc_opt)

checkresiduals(ets_fc_opt)


ets_opt <- forecast(ets_fc_opt, h = length(test))

accuracy(ets_opt, test)
#  Smoothing parameters:
#    alpha = 0.7558 
#    beta  = 0.0062 
#    gamma = 1e-04 
#    phi   = 0.9777 
```

```{r}
ets_f1 <- forecast(ets_aaa, h = length(test))
accuracy(ets_f1, test)
#                     ME      RMSE       MAE        MPE      MAPE      MASE
# Training set   4720.584  46864.94  34369.72  0.3300935  3.302626 0.5532804
# Test set     -94545.374 171051.47 135512.58 -9.7492477 11.714521 2.1814680
#     AIC     AICc      BIC 
# 3353.638 3359.305 3401.855 

ets_f2 <- forecast(ets_mmm, h = length(test))
accuracy(ets_f2, test)
#                    ME     RMSE      MAE       MPE     MAPE      MASE
# Training set  4669.134 35897.27 25308.33 0.3287808 2.394707 0.4074109
# Test set     55354.659 84627.46 58476.46 3.5078162 3.680772 0.9413483
#     AIC     AICc      BIC 
# 3272.836 3279.228 3323.889 

ets_f3 <- forecast(ets_maa, h = length(test))
accuracy(ets_f3, test)
#                     ME      RMSE       MAE        MPE      MAPE     MASE
# Training set   4410.507  48254.26  35326.47  0.2640174  3.419934 0.568682
# Test set     -59309.069 157440.58 128378.53 -7.2038709 10.626490 2.066625
#     AIC     AICc      BIC 
# 3354.773 3361.166 3405.826 


ets_f4 <- forecast(ets_aam, h = length(test))
accuracy(ets_f4, test)
#                    ME     RMSE      MAE       MPE     MAPE      MASE
# Training set  3842.729 35628.41 24905.46 0.3159739 2.376534 0.4009256
# Test set     39822.985 70208.79 48639.90 2.4979209 3.038618 0.7830002
#     AIC     AICc      BIC 
#3286.558 3292.950 3337.611 

ets_f5 <- forecast(ets_mam, h = length(test))
accuracy(ets_f5, test)
#                   ME     RMSE      MAE       MPE     MAPE      MASE
# Training set  4714.92 35958.65 25281.32 0.3450024 2.393413 0.4069762
# Test set     59890.33 89410.83 62377.01 3.8130139 3.946629 1.0041388
#     AIC     AICc      BIC 
#3273.247 3279.639 3324.300

```

Calling the "ZZZ" gave us optimal model for MAM, however AAM looks like it has the lowest errors and best accuracy across the models. Lets try to optimize this model by finding the best gamma. 


#### optimizing the gamma
We will attempt to optimize the parameter in our Holt-Winters model. Here, we use the AAM model that minimized our prediction errors above (MAPE) and identify the gamma parameter that minimizes forecast errors. In this case we see that gamma = 0.06 minimizes the error rate.
```{r}

gamma <- seq(0.01, 0.85, 0.01)
RMSE <- NA

for(i in seq_along(gamma)) {
  hw.expo <- ets(train, "AAM", restrict = FALSE, gamma = gamma[i])
  future <- forecast(hw.expo, h = length(test))
  RMSE[i] = accuracy(future, test)[2,2]
}

error <- data_frame(gamma, RMSE)
minimum <- filter(error, RMSE == min(RMSE))
ggplot(error, aes(gamma, RMSE)) +
  geom_line() +
  geom_point(data = minimum, color = "blue", size = 2) +
  ggtitle("gamma's impact on forecast errors",
          subtitle = "gamma = 0.06 minimizes RMSE")


```

```{r}
# print original model for comparison
accuracy(ets_f4, test)
#                    ME     RMSE      MAE       MPE     MAPE      MASE
# Training set  3842.729 35628.41 24905.46 0.3159739 2.376534 0.4009256
# Test set     39822.985 70208.79 48639.90 2.4979209 3.038618 0.7830002


# fit model with optimal gamma parameter
ets_aam_opt_gamma <- ets(train, model = "AAM", restrict = FALSE, gamma = 0.06)
ets_f6 <- forecast(ets_aam_opt_gamma, h = length(test))

# print new model for comparison
accuracy(ets_f6, test)
#                   ME     RMSE      MAE       MPE     MAPE      MASE
# Training set 3071.692 36992.85 26041.99 0.1720018 2.516764 0.4192214
# Test set     6848.922 30226.03 25549.42 0.3100556 1.738506 0.4112920

autoplot(ets_f6) + ggtitle("ETS (A,A,M) Forecast")

```
Updating our model with this “optimal” gamma 0.06, we see that we bring our forecasting error rate down from 3.04% to 1.74%, tho the test error should be worse than the training error so this model is over fit.

```{r}

gamma <- seq(0.01, 0.85, 0.01)
RMSE <- NA

for(i in seq_along(gamma)) {
  hw.expo <- ets(train, "MAM", gamma = gamma[i])
  future <- forecast(hw.expo, h = length(test))
  RMSE[i] = accuracy(future, test)[2,2]
}

error <- data_frame(gamma, RMSE)
minimum2 <- filter(error, RMSE == min(RMSE))
ggplot(error, aes(gamma, RMSE)) +
  geom_line() +
  geom_point(data = minimum2, color = "blue", size = 2) +
  ggtitle("gamma's impact on forecast errors",
          subtitle = "gamma = 0.84 minimizes RMSE")
```


```{r}
minimum2

accuracy(ets_f5, test)
#                   ME     RMSE      MAE       MPE     MAPE      MASE     
#Training set  4714.92 35958.65 25281.32 0.3450024 2.393413 0.4069762
#Test set     59890.33 89410.83 62377.01 3.8130139 3.946629 1.0041388 

# new model with optimal gamma parameter
ets_mam_opt_gamma <- ets(train, model = "MAM", gamma = 0.84)
ets_f7 <- forecast(ets_mmm_opt_gamma, h = length(test))
accuracy(ets_f7, test)
#                    ME     RMSE      MAE       MPE     MAPE      MASE
# Training set  4230.297 36196.51 25331.83 0.2775123 2.391711 0.4077893 
# Test set     46685.266 76473.64 51304.06 2.8677488 3.190744 0.8258876

autoplot(ets_f7) + ggtitle("ETS (M,A,M) Forecast")
```

Updating our model with this “optimal” gamma 0.84, we see that we bring our forecasting error rate down from 3.95% to 3.19%.

```{r}

accuracy(ets_f7, test)
#                    ME     RMSE      MAE       MPE     MAPE      MASE
# Training set  4230.297 36196.51 25331.83 0.2775123 2.391711 0.4077893 
# Test set     46685.266 76473.64 51304.06 2.8677488 3.190744 0.8258876
# lets try damping with the optimal model

ets_mmm_opt_damp <- ets(train, model = "MAM",  damped = TRUE, gamma = 0.84, phi = 0.85)
ets_f8 <- forecast(ets_mmm_opt_damp, h = length(test))
accuracy(ets_f8, test)
#                    ME     RMSE      MAE         MPE     MAPE      MASE 
#Training set  6975.356 40755.04 31173.21  0.36865882 2.953685 0.5018232 
#Test set     -1457.605 62414.95 50371.63 -0.07026508 2.998235 0.8108775 
```

Damping reduced our testing error but increased our training error.

```{r}

# out of all the ETS() models,  ETS(M,A,M) damped model works the best

# check out the model
autoplot(ets_mmm_opt_damp)

# check residuals and one-step forecast errors from the ETS(M,A,M) damped model.
cbind('Residuals' = residuals(ets_mmm_opt_damp),
      'Forecast errors' = residuals(fit,type='response')) %>%
  autoplot(facet=TRUE) + xlab("Year") + ylab("")

# check residuals
checkresiduals(ets_mmm_opt_damp)

```

Looks close to white noise...

```{r}

ets_f8 %>% forecast(h=length(test)) %>%
  autoplot(facets = TRUE) + ggtitle("South EU Passengers | 1 YR Forecast | ETS (M,A,M) - Damped") + ylab("Passengers") + xlab("Time")

#                    ME     RMSE      MAE         MPE     MAPE      MASE 
#Training set  6975.356 40755.04 31173.21  0.36865882 2.953685 0.5018232 
#Test set     -1457.605 62414.95 50371.63 -0.07026508 2.998235 0.8108775 
```

This is the best ETS() model we have, ETS (M,A,M) - Damped


### TBATS()

```{r}
# fit tbats
tbats_fit <- tbats(train)

# forecast
tbats_fc <- forecast(tbats_fit, h =length(test)) 

# plot and summarize the forecast
autoplot(tbats_fc) + ggtitle("TBATS Forecast - TBATS(0.001, {0,0}, -, {<12,5>})")
summary(tbats_fc)

# check residuals
checkresiduals(tbats_fc)

# check accuracy
accuracy(tbats_fc, test)
#                    ME      RMSE      MAE       MPE     MAPE      MASE        ACF1 Theil's U
# Training set  6172.75  37654.02 26146.16 0.3534106 2.447341 0.4208982 -0.04514903        NA
# Test set     63821.39 100214.35 67404.42 3.9672241 4.141832 1.0850697  0.49269555 0.3172993

# Forecast method: TBATS(0.001, {0,0}, -, {<12,5>})
```




###  End Lola's Section 




#### Arima (Jerry) 

#### auto.arima (Julian)
```{r}
# fit auto.arima() model
auto_arima_model = auto.arima(train, stationary=FALSE, seasonal=TRUE)

# summary
summary(auto_arima_model)
```

```{r}
# investigate residuals
checkresiduals(auto_arima_model)
```
\
- The residuals for the plots do not appear to be white noise\
- However, based on the Ljung-Box test, we cannot accept the null hypothesis of the data to be independent  since the p-value < 0.05\
- At lag 11 there is a spice in the ACF plot for the residuals\

```{r}
# forecast
h = 12
auto_arima_fc = forecast(auto_arima_model, h=h)
```

```{r}
# plot test data and forecasts of both models
autoplot(test_xts, xlab = "Time", ylab = "Number of passengers",
         main="12-month forecast for South Europe Monthly with auto arima model", series="Test data") +
  autolayer(auto_arima_fc$mean, series="Forecast model_1")
```


```{r}
library(xts)
test_xts <- as.xts(test)


str(test_xts)

autoplot(test_xts)
```




```{r}
# plot the forecast
plot(auto_arima_fc, main="12-month forecast for South Europe Monthly with auto arima model")
```
\
- In this plot we can observe the multiple seasonality in the data for South Europe

```{r}
# accuracy
accuracy(auto_arima_fc, test)

# save RMSE/MAE
auto_arima_rmse_test = accuracy(auto_arima_fc, test)["Test set", "RMSE"]
auto_arima_rmse_test = accuracy(auto_arima_fc, test)["Test set", "MAE"]
```


#### VAR (Julian)
```{r}
# source https://datahub.io/core/cpi-us#r
json_file = 'https://datahub.io/core/cpi-us/datapackage.json'
json_data = fromJSON(paste(readLines(json_file), collapse=""))

# get list of all resources:
print(json_data$resources$name)

# print all tabular data(if exists any)
for(i in 1:length(json_data$resources$datahub$type)){
  if(json_data$resources$datahub$type[i]=='derived/csv'){
    path_to_file = json_data$resources$path[i]
    dat_cpi = read.csv(url(path_to_file))
    print(head(dat_cpi))
  }
}

# create year column
dat_cpi["year"] = substr(dat_cpi[,1], start = 1, stop = 4)

# create month column
dat_cpi["month"] = substr(dat_cpi[,1], start = 6, stop = 7)

# adjust type
dat_cpi["month"] = as.numeric(unlist(dat_cpi["month"]))
dat_cpi["year"] = as.numeric(unlist(dat_cpi["year"]))

# impute mean for nas in cpi and inflation
dat_cpi[is.na(dat_cpi[,2]), 2] <- mean(dat_cpi[,2], na.rm = TRUE)
dat_cpi[is.na(dat_cpi[,3]), 3] <- mean(dat_cpi[,3], na.rm = TRUE)

# create ts object for cpi
cpi_ts = ts(dat_cpi["Index"], frequency = 12, start=c(1990,1), end=c(2019,6))

# create ts object for inflation
infl_ts = ts(dat_cpi["Inflation"], frequency = 12, start=c(1990,1), end=c(2019,6))
```

```{r}
# plot cpi and inflation
plot(cpi_ts, main="CPI in the US from 1900 to 2020")
plot(infl_ts, main="Inflation in the US from 1900 to 2020")
```

```{r}
# select test and training data
train_cpi = window(cpi_ts, c(2008,1), c(2018,6))
test_cpi = window(cpi_ts, c(2018,7))

# var order selection for number of passengers
VARselect(cbind(south_europe_ts_m, cpi_ts), lag.max = 15, type = "const")$selection

# build model
var_model_1 = VAR(cbind(train, train_cpi), p=4, type="both", season=12)
var_model_2 = VAR(cbind(train, train_cpi), p=13, type="both", season=12)

# forecast
h = 12
var_fc_1 = forecast(var_model_1, h=h)
var_fc_2 = forecast(var_model_2, h=h)
```
\
- Based on the AIC, the VAR(13) should be selected\

```{r}
# test serial correlation in the residuals
serial.test(var_model_1, lags.pt = 10, type = "PT.asymptotic")

# plot acf of residuals
varresids_1 = residuals(var_model_1)
acf(varresids_1[,1], main="Residuals Number Passengers")
acf(varresids_1[,2], main="Residuals CPI")

# test serial correlation in the residuals
serial.test(var_model_2, lags.pt = 10, type = "PT.asymptotic")

# plot acf of residuals
varresids_2 = residuals(var_model_2)
acf(varresids_2[,1], main="Residuals Number Passengers")
acf(varresids_2[,2], main="Residuals CPI")
```
\
- The null hypothesis of the serial.test is that there is no serial correlation in the residuals\
- For a VAR(13) model, the null hypothesis of the serial.test is rejected\
- The ACF for the residuals of the number of passengers also shows that there is some pattern in the residuals\
- A VAR(4) model results in the ability to not reject the null hypothesis for residuals with serial correlation with a p-value > 0.05, but the ACF plot still suggest some pattern in the residuals\

```{r}
# plot test data and forecasts 
autoplot(test, xlab = "Time", ylab = "Number of passengers",
         main="12-month forecast for South Europe Monthly with VAR(4) model", series="Test data") +
  autolayer(var_fc_1$forecast$train$mean, series="Forecast VAR(4) model")
```


```{r}
# accuracy
accuracy(var_fc_1$forecast$train, test)

# save RMSE/MAE
var_rmse_test = accuracy(var_fc_1$forecast$train, test)["Test set", "RMSE"]
var_rmse_test = accuracy(var_fc_1$forecast$train, test)["Test set", "MAE"]
```


#### TBATS (Lola)
```{r}
# fit tbats model
tbats_model = tbats(train)

# summary
summary(tbats_model)

# investigate residuals
checkresiduals(tbats_model)
```

#### Multiple seasonality (Kelley)
```{r}
# fit auto.arima model with fourier
multiple_seasonality_model = auto.arima(train, xreg=fourier(train, K=5), seasonal=FALSE)

# summary
summary(multiple_seasonality_model)

# investigate residuals
checkresiduals(multiple_seasonality_model)
```
\
- Compared to model_1, the p-value for the Ljung-Box test is higher, meaning that the model is better in capturing the patterns in the residuals, but it is not capturing it sufficiently well since we can still not accept the null hypothesis of uncorrelated residuals\

```{r}
# forecast into the future
multiple_seasonality_fc = forecast(multiple_seasonality_model, xreg = fourier(train, K=5, h=24), level=c(80, 95))

# plot the forecast
plot(multiple_seasonality_fc, 
     main="24-month forecast for South Europe Monthly with auto arima model with fourier term")
```










